{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLCJYchKp+0JVpd5xjv3b+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LearnByDoing2024/Youtube/blob/main/20241029_hugg_model_test/huggingface_model_tests_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a structured workflow to test various types of Hugging Face models in Colab, passing outputs from one to another. We will use lightweight models that can run smoothly on Colab’s CPU. I’ve designed the flow to include different model types like text generation, question answering, summarization, translation, and image captioning.\n",
        "\n",
        "### 1. **Install Necessary Libraries**\n",
        "You’ll need the `transformers` and `datasets` libraries from Hugging Face. Let’s install them first.\n",
        "\n",
        "```bash\n",
        "!pip install transformers datasets\n",
        "```\n",
        "\n",
        "### 2. **Define the Models and Pipeline Types**\n",
        "\n",
        "We will use the following lightweight models to test:\n",
        "- **Text Generation**: GPT-2 (small)\n",
        "- **Summarization**: t5-small\n",
        "- **Question Answering**: DistilBERT (small and fast)\n",
        "- **Translation**: Helsinki-NLP’s Opus-MT for English to French\n",
        "- **Sentiment Analysis**: DistilBERT for sentiment classification\n",
        "\n",
        "### 3. **Information Flow Design**\n",
        "\n",
        "1. **Start with Text Generation**: Use GPT-2 to generate text based on an initial prompt.\n",
        "2. **Summarize the Generated Text**: Use t5-small to summarize the generated text.\n",
        "3. **Answer a Question Based on the Summarized Text**: Feed the summarized text into a question-answering model.\n",
        "4. **Translate the Answer**: Translate the answer from English to French.\n",
        "5. **Perform Sentiment Analysis**: Classify the sentiment of the final translated response.\n",
        "\n",
        "### 4. **Colab Code to Implement the Flow**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Text Generation (GPT-2)\n",
        "generator = pipeline('text-generation', model='gpt2', max_length=50)\n",
        "initial_prompt = \"The future of artificial intelligence is\"\n",
        "generated_text = generator(initial_prompt)[0]['generated_text']\n",
        "\n",
        "print(f\"Generated Text: {generated_text}\")\n",
        "\n",
        "# 2. Summarization (t5-small)\n",
        "summarizer = pipeline('summarization', model='t5-small', max_length=50)\n",
        "summarized_text = summarizer(generated_text)[0]['summary_text']\n",
        "\n",
        "print(f\"Summarized Text: {summarized_text}\")\n",
        "\n",
        "# 3. Question Answering (DistilBERT)\n",
        "qa = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
        "question = \"What is the text about?\"\n",
        "qa_output = qa(question=question, context=summarized_text)['answer']\n",
        "\n",
        "print(f\"Question Answering Output: {qa_output}\")\n",
        "\n",
        "# 4. Translation (Helsinki-NLP, English to French)\n",
        "translator = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\n",
        "translated_output = translator(qa_output)[0]['translation_text']\n",
        "\n",
        "print(f\"Translated Output (French): {translated_output}\")\n",
        "\n",
        "# 5. Sentiment Analysis (DistilBERT)\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "sentiment = sentiment_analyzer(translated_output)[0]['label']\n",
        "\n",
        "print(f\"Sentiment of the Translated Text: {sentiment}\")\n",
        "```\n",
        "\n",
        "### 5. **Explanation of Each Step**\n",
        "\n",
        "1. **Text Generation**: GPT-2 generates a continuation of the prompt. It’s lightweight enough to run efficiently on Colab’s CPU.\n",
        "2. **Summarization**: The `t5-small` model summarizes the generated text to keep the flow concise and manageable.\n",
        "3. **Question Answering**: We use DistilBERT to answer a question about the summarized text, a useful way to interact with the information.\n",
        "4. **Translation**: The Opus-MT model translates the answer from English to French, providing a multilingual perspective.\n",
        "5. **Sentiment Analysis**: Finally, DistilBERT classifies the sentiment of the translated text, giving an extra layer of analysis.\n",
        "\n",
        "### 6. **Customizing the Information Flow**\n",
        "\n",
        "You can change the initial prompt, the question, or even add more models like text classification or entity recognition to extend the workflow further.\n",
        "\n",
        "Let me know if you need additional adjustments or explanations for any part of the workflow!"
      ],
      "metadata": {
        "id": "P9cN-rAXgG38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a comparison table specifically for the models you've chosen, focusing on their technical specifications and characteristics:\n",
        "\n",
        "| **Model Name**              | **Task**               | **Parameters**  | **Model Size**  | **Architecture**            | **Notable Characteristics**                                      | **Tech Requirements**            |\n",
        "|-----------------------------|------------------------|-----------------|----------------|-----------------------------|------------------------------------------------------------------|-----------------------------------|\n",
        "| **GPT-2 (small)**            | Text Generation        | 124M            | ~500MB          | Transformer (decoder-only)   | Good for text generation, fast, produces fluent continuations     | Runs well on CPU                 |\n",
        "| **t5-small**                 | Summarization          | 60M             | ~231MB          | Transformer (encoder-decoder)| Versatile across tasks (summarization, translation, etc.), lightweight | CPU/GPU compatible               |\n",
        "| **DistilBERT (small)**       | Question Answering     | 66M             | ~255MB          | Transformer (encoder)        | Efficient version of BERT, quick QA, very fast and lightweight    | Highly efficient on CPU          |\n",
        "| **Helsinki-NLP Opus-MT**     | Translation (EN to FR) | ~75M            | ~300MB          | Transformer (encoder-decoder)| Specialized in language translation, supports low-resource languages | Works efficiently on CPU/GPU     |\n",
        "| **DistilBERT (sentiment)**   | Sentiment Analysis     | 66M             | ~255MB          | Transformer (encoder)        | Fast and efficient for sentiment classification                  | CPU friendly, fast inference     |\n",
        "\n",
        "### Key Model Characteristics:\n",
        "\n",
        "1. **GPT-2 (small)**:\n",
        "   - **Text Generation**: This decoder-only model is great for generating fluent and coherent continuations of text prompts.\n",
        "   - **Parameters**: 124M, making it lightweight and fast compared to larger GPT-2 models.\n",
        "   - **Use Case**: Ideal for generating short to medium-length text for tasks like story generation or code completion.\n",
        "   \n",
        "2. **t5-small**:\n",
        "   - **Summarization**: T5 is a highly flexible encoder-decoder model, and `t5-small` is a compact version optimized for low-resource tasks.\n",
        "   - **Parameters**: 60M, making it lightweight for handling summarization, translation, and classification tasks.\n",
        "   - **Use Case**: Best suited for quick summarization tasks or translation without consuming too much memory.\n",
        "   \n",
        "3. **DistilBERT (small)**:\n",
        "   - **Question Answering**: DistilBERT is a distilled version of BERT, optimized for speed and efficiency, perfect for tasks like question answering.\n",
        "   - **Parameters**: 66M, which is 40% fewer than BERT-base but retains 97% of its performance.\n",
        "   - **Use Case**: Ideal for real-time question-answering systems, especially where resource constraints exist.\n",
        "   \n",
        "4. **Helsinki-NLP’s Opus-MT**:\n",
        "   - **Translation (EN to FR)**: This model is designed for translation tasks and supports multiple language pairs, making it versatile for low-resource languages.\n",
        "   - **Parameters**: 75M, lightweight for a translation model.\n",
        "   - **Use Case**: Excellent for translating text from English to French (and other supported language pairs), useful in multilingual applications.\n",
        "   \n",
        "5. **DistilBERT (Sentiment Analysis)**:\n",
        "   - **Sentiment Analysis**: Another lightweight version of BERT, this one fine-tuned for text classification tasks like sentiment analysis.\n",
        "   - **Parameters**: 66M, allowing it to run quickly while maintaining good accuracy.\n",
        "   - **Use Case**: Perfect for real-time sentiment classification of social media posts, product reviews, and feedback.\n",
        "\n",
        "### Why These Models are Suitable for Colab's CPU:\n",
        "- **Small and Efficient**: All these models are compact enough to run smoothly on Colab’s CPU without overwhelming the environment's resources.\n",
        "- **Multi-Tasking**: Most of these models (T5, DistilBERT) are versatile and can handle a variety of NLP tasks, making them ideal for experimentation in low-resource settings.\n",
        "- **Faster Inference**: Their lightweight nature ensures that you can quickly run multiple experiments, even in environments where a GPU isn't available.\n",
        "\n",
        "This setup will ensure fast response times, even when working purely with CPUs, making these models perfect for your Colab workflow."
      ],
      "metadata": {
        "id": "7iYoRj4KlAvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "HufQC3xugXf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dri-E4aygC1G",
        "outputId": "10df3648-9f3c-4249-f375-aaf1a2704916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: The future of artificial intelligence is in question, particularly with robots on an increasingly large scale. There are no known scientific approaches to improving AI, and no real evidence of how a robot could improve that.\n",
            "\n",
            "But the results of one of the world's most comprehensive research into AI and artificial intelligence — IBM's Watson — highlight a crucial difference between what Watson and machine learning do and what is increasingly important for the future of AI and robotics.\n",
            "\n",
            "IBM's Watson is a sophisticated computer that uses\n"
          ]
        }
      ],
      "source": [
        "# 1. Text Generation (GPT-2)\n",
        "generator = pipeline('text-generation', model='gpt2', max_length=100)\n",
        "initial_prompt = \"The future of artificial intelligence is\"\n",
        "generated_text = generator(initial_prompt)[0]['generated_text']\n",
        "\n",
        "print(f\"Generated Text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Summarization (t5-small)\n",
        "summarizer = pipeline('summarization', model='t5-small', max_length=50)\n",
        "summarized_text = summarizer(generated_text)[0]['summary_text']\n",
        "\n",
        "print(f\"Summarized Text: {summarized_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBfHakeYgMuM",
        "outputId": "0ddf3254-43f6-4925-bdcf-20a734277923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized Text: the future of artificial intelligence is in question, particularly with robots on an increasingly large scale . there are no known scientific approaches to improving AI, and no real evidence of how a robot could improve that . but the results of one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Question Answering (DistilBERT)\n",
        "qa = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
        "question = \"What is the text about?\"\n",
        "qa_output = qa(question=question, context=summarized_text)['answer']\n",
        "\n",
        "print(f\"Question Answering Output: {qa_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI4Ws8STgOWL",
        "outputId": "1cc5b40f-9d37-4188-95c7-6766e879e5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Answering Output: the future of artificial intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Translation (Helsinki-NLP, English to French)\n",
        "translator = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\n",
        "translated_output = translator(qa_output)[0]['translation_text']\n",
        "\n",
        "print(f\"Translated Output (French): {translated_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isNlI66ygPZ7",
        "outputId": "6b6b058b-985a-4297-c52e-8b60d0cace76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Output (French): l'avenir de l'intelligence artificielle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Sentiment Analysis (DistilBERT)\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "sentiment = sentiment_analyzer(translated_output)[0]['label']\n",
        "\n",
        "print(f\"Sentiment of the Translated Text: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8BtpibPgQfd",
        "outputId": "893d858d-a79b-4e92-cde2-8c18fbf20eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of the Translated Text: POSITIVE\n"
          ]
        }
      ]
    }
  ]
}